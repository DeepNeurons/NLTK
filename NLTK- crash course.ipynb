{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c781ac",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a35ae3",
   "metadata": {},
   "source": [
    "## 1. Tokenizing text into sentences\n",
    "\n",
    "* Text Tokenizing is the process of splitting text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4effbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing (NLP) refers to the branch of computer science. More specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6237a",
   "metadata": {},
   "source": [
    "* <b>Methode 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb3cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## intsall NLTK , note that NLTK stands for Natural Language Toolkit\n",
    "# !pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d328942a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokens = sent_tokenize(text)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb25c8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) refers to the branch of computer science.',\n",
       " 'More specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokens are list of sentences \n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e2382",
   "metadata": {},
   "source": [
    "* <b>Methode 2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc064b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) refers to the branch of computer science.',\n",
       " 'More specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can go also with a pretrained tokenizer \n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') # ML model for text tokenizer\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44468222",
   "metadata": {},
   "source": [
    "-> Note the two methodes produce the same result but with a different latency (more latency using method 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936637e",
   "metadata": {},
   "source": [
    "## 2. Tokenizing sentences into words\n",
    "* splitting sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf053676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science', '.']\n",
      "['More', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI—concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "## tokens are the tokenized sentences we had in the previous cells\n",
    "for sent in tokens:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a035a9",
   "metadata": {},
   "source": [
    "## 3 .STOP words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaac6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6403933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23944917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Edi',\n",
       " 'big',\n",
       " 'fan',\n",
       " 'real',\n",
       " 'madrid',\n",
       " 'football',\n",
       " 'team',\n",
       " ',',\n",
       " 'however',\n",
       " 'fond',\n",
       " 'tennis',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Edi is a big fan of real madrid football team,however he is not too fond of tennis.\"\n",
    "# tokenizing text into words\n",
    "tokens = word_tokenize(text)\n",
    "#removing stop words from it\n",
    "clean_tokens = [word for word in tokens if not word in stopwords.words('english')]\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd8c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note : stop words arer available in almost every language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f940ec",
   "metadata": {},
   "source": [
    "## 4. Stemming\n",
    "\n",
    "- \"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\"__ datacamp.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934cc4e",
   "metadata": {},
   "source": [
    "* <b> Method 1 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb38f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stem of the world 'Eating' is : eat\n",
      "the stem of the world 'swimming' is : swim\n",
      "the stem of the world 'amazing' is : amaz\n",
      "the stem of the world 'amazon' is : amazon\n",
      "the stem of the world 'fairly' is : fairli\n"
     ]
    }
   ],
   "source": [
    "# import nltk \"Porterstemmer\" package\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "## test with few examples : Eating, swimming, amazing,amazon,fairly\n",
    "print(f\"the stem of the world 'Eating' is : {porter.stem('Eating')}\")\n",
    "print(f\"the stem of the world 'swimming' is : {porter.stem('swimming')}\")\n",
    "print(f\"the stem of the world 'amazing' is : {porter.stem('amazing')}\")\n",
    "print(f\"the stem of the world 'amazon' is : {porter.stem('amazon')}\")\n",
    "print(f\"the stem of the world 'fairly' is : {porter.stem('fairly')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030aa292",
   "metadata": {},
   "source": [
    "* let's try with another stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec9246",
   "metadata": {},
   "source": [
    "* <b> Method 2 </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d71ed9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stem of the world 'Eating' is : eat\n",
      "the stem of the world 'swimming' is : swim\n",
      "the stem of the world 'amazing' is : amaz\n",
      "the stem of the world 'amazon' is : amazon\n",
      "the stem of the world 'fairly' is : fair\n"
     ]
    }
   ],
   "source": [
    "# import SnowballStemmer \n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "print(f\"the stem of the world 'Eating' is : {snowball.stem('Eating')}\")\n",
    "print(f\"the stem of the world 'swimming' is : {snowball.stem('swimming')}\")\n",
    "print(f\"the stem of the world 'amazing' is : {snowball.stem('amazing')}\")\n",
    "print(f\"the stem of the world 'amazon' is : {snowball.stem('amazon')}\")\n",
    "print(f\"the stem of the world 'fairly' is : {snowball.stem('fairly')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aef811",
   "metadata": {},
   "source": [
    "-> Notice that the word fairly was stemmed correctly with snowballStemmer and incorrectly with PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a7a7f",
   "metadata": {},
   "source": [
    "* <b> Method 3 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d0cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stem of the world 'Eating' is : eat\n",
      "the stem of the world 'swimming' is : swim\n",
      "the stem of the world 'amazing' is : amaz\n",
      "the stem of the world 'amazon' is : amazon\n",
      "the stem of the world 'fairly' is : fair\n"
     ]
    }
   ],
   "source": [
    "# import LancasterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "print(f\"the stem of the world 'Eating' is : {lancaster.stem('Eating')}\")\n",
    "print(f\"the stem of the world 'swimming' is : {lancaster.stem('swimming')}\")\n",
    "print(f\"the stem of the world 'amazing' is : {lancaster.stem('amazing')}\")\n",
    "print(f\"the stem of the world 'amazon' is : {lancaster.stem('amazon')}\")\n",
    "print(f\"the stem of the world 'fairly' is : {lancaster.stem('fairly')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce49a6",
   "metadata": {},
   "source": [
    "## 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97777886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking --> cook\n",
      "buses --> bus\n",
      "loving --> love\n"
     ]
    }
   ],
   "source": [
    "## import lemmatizer package from NLTK\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(f\"cooking --> {lemmatizer.lemmatize('cooking', pos='v')}\")\n",
    "print(f\"buses --> {lemmatizer.lemmatize('buses', pos='v')}\")\n",
    "print(f\"loving --> {lemmatizer.lemmatize('loving', pos='v')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46841e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a3a1755",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging \n",
    "\n",
    "* \"Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context.\"___towardsdatascience by Kurtis Pykes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa834e",
   "metadata": {},
   "source": [
    "NLTK POS Tags Examples are as below: (Source : <b>guru99.com</b>)\n",
    "\n",
    "<table ><tr><th>Abbreviation</th><th>Meaning</th></tr>\n",
    "\n",
    "<tr ><td><span >CC</span></td><td>coordinating conjunction</td>\n",
    "\n",
    "<tr ><td><span >CD</span></td><td>cardinal digit</td>\n",
    "<tr ><td><span >DT\t</span></td><td>determiner</td>\n",
    "<tr ><td><span >EX\t</span></td><td>existential there</td>\n",
    "<tr ><td><span >FW\t</span></td><td>foreign word</td>\n",
    "<tr ><td><span >IN\t</span></td><td>preposition/subordinating conjunction</td>\n",
    "<tr ><td><span >JJ\t</span></td><td>This NLTK POS Tag is an adjective (large)</td>\n",
    "<tr ><td><span >JJR\t</span></td><td>adjective, comparative (larger)</td>\n",
    "<tr ><td><span >JJS\t</span></td><td>adjective, superlative (largest)</td>\n",
    "<tr ><td><span >LS\t</span></td><td>list market</td>\n",
    "<tr ><td><span >MD\t</span></td><td>modal (could, will)</td>\n",
    "<tr ><td><span >NN\t</span></td><td>noun, singular (cat, tree)</td>\n",
    "<tr ><td><span >NNS\t</span></td><td>noun plural (desks)</td>\n",
    "<tr ><td><span >NNP\t</span></td><td>proper noun, singular (sarah)</td>\n",
    "<tr ><td><span >NNPS\t</span></td><td>proper noun, plural (indians or americans)</td>\n",
    "<tr ><td><span >PDT\t</span></td><td>predeterminer (all, both, half)</td>\n",
    "<tr ><td><span >POS\t</span></td><td>possessive ending (parent\\ ‘s)</td>\n",
    "<tr ><td><span >PRP\t</span></td><td>personal pronoun (hers, herself, him, himself)</td>\n",
    "<tr ><td><span > PRP$\t</span></td><td> possessive pronoun (her, his, mine, my, our )</td>\n",
    "<tr ><td><span > RB\t</span></td><td>adverb (occasionally, swiftly)</td>\n",
    "<tr ><td><span >RBR\t</span></td><td>adverb, comparative (greater)</td>\n",
    "<tr ><td><span >RBS\t</span></td><td>adverb, superlative (biggest)</td>\n",
    "<tr ><td><span >RP\t</span></td><td>particle (about)</td>\n",
    "<tr ><td><span >TO\t</span></td><td>infinite marker (to)</td>\n",
    "<tr ><td><span >UH\t</span></td><td>interjection (goodbye)</td>\n",
    "<tr ><td><span >VB\t</span></td><td>verb (ask)</td>\n",
    "<tr ><td><span >VBG\t</span></td><td>verb gerund (judging)</td>\n",
    "<tr ><td><span >VBD\t</span></td><td>verb past tense (pleaded)</td>\n",
    "<tr ><td><span >VBN\t</span></td><td>verb past participle (reunified)</td>\n",
    "<tr ><td><span >VBP\t</span></td><td>verb, present tense not 3rd person singular(wrap)</td>\n",
    "<tr ><td><span >VBZ\t</span></td><td>verb, present tense with 3rd person singular (bases)</td>\n",
    "<tr ><td><span >WDT\t</span></td><td>wh-determiner (that, what)</td>\n",
    "<tr ><td><span >WP\t</span></td><td>wh- pronoun (who)</td>\n",
    "<tr ><td><span >WRB\t</span></td><td>wh- adverb (how)</td>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80117ff",
   "metadata": {},
   "source": [
    "## 1.Default Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f344d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'Tag1'), ('World!', 'Tag1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "Tagger = DefaultTagger('Tag1')\n",
    "Tagger.tag(['Hello','World!'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
